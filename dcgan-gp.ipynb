{"cells":[{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-01-31T07:18:14.713012Z","iopub.status.busy":"2024-01-31T07:18:14.712752Z","iopub.status.idle":"2024-01-31T07:18:14.718909Z","shell.execute_reply":"2024-01-31T07:18:14.718014Z","shell.execute_reply.started":"2024-01-31T07:18:14.712989Z"},"trusted":true},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import os\n","import time\n","from keras.optimizers import Adam, SGD\n","from keras.models import Sequential\n","from keras.layers import Dense, Reshape, Flatten,Conv2D,Conv2DTranspose,LeakyReLU,Dropout,UpSampling2D, BatchNormalization, Input, GaussianNoise\n","from keras.activations import tanh\n","import tensorflow as tf\n","import keras\n","import random"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-01-31T07:18:18.883642Z","iopub.status.busy":"2024-01-31T07:18:18.883290Z","iopub.status.idle":"2024-01-31T07:18:18.891587Z","shell.execute_reply":"2024-01-31T07:18:18.890616Z","shell.execute_reply.started":"2024-01-31T07:18:18.883614Z"},"trusted":true},"outputs":[],"source":["@keras.saving.register_keras_serializable(package=\"InstanceNormalization\")\n","class InstanceNormalization(tf.keras.layers.Layer):\n","  \"\"\"Instance Normalization Layer (https://arxiv.org/abs/1607.08022).\"\"\"\n","\n","  def __init__(self, epsilon=1e-5):\n","    super(InstanceNormalization, self).__init__()\n","    self.epsilon = epsilon\n","\n","  def build(self, input_shape):\n","    self.scale = self.add_weight(\n","        name='scale',\n","        shape=input_shape[-1:],\n","        initializer=tf.random_normal_initializer(1., 0.02),\n","        trainable=True)\n","\n","    self.offset = self.add_weight(\n","        name='offset',\n","        shape=input_shape[-1:],\n","        initializer='zeros',\n","        trainable=True)\n","\n","  def call(self, x):\n","    mean, variance = tf.nn.moments(x, axes=[1, 2], keepdims=True)\n","    inv = tf.math.rsqrt(variance + self.epsilon)\n","    normalized = (x - mean) * inv\n","    return self.scale * normalized + self.offset\n","\n"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-01-31T07:18:21.659545Z","iopub.status.busy":"2024-01-31T07:18:21.658659Z","iopub.status.idle":"2024-01-31T07:18:21.663868Z","shell.execute_reply":"2024-01-31T07:18:21.662799Z","shell.execute_reply.started":"2024-01-31T07:18:21.659510Z"},"trusted":true},"outputs":[],"source":["# parameers \n","Batch_size=64\n","IMG_H=128\n","IMG_W=128\n","learning_rate=0.0001\n","b1=0.0\n","latent_dim=100\n"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-01-31T07:18:24.259541Z","iopub.status.busy":"2024-01-31T07:18:24.258677Z","iopub.status.idle":"2024-01-31T07:18:25.509266Z","shell.execute_reply":"2024-01-31T07:18:25.508432Z","shell.execute_reply.started":"2024-01-31T07:18:24.259504Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/keras/src/initializers/initializers.py:120: UserWarning: The initializer RandomNormal is unseeded and being called multiple times, which will return identical values each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initializer instance more than once.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," dense (Dense)               (None, 8192)              827392    \n","                                                                 \n"," leaky_re_lu (LeakyReLU)     (None, 8192)              0         \n","                                                                 \n"," reshape (Reshape)           (None, 4, 4, 512)         0         \n","                                                                 \n"," conv2d_transpose (Conv2DTr  (None, 8, 8, 512)         4194304   \n"," anspose)                                                        \n","                                                                 \n"," instance_normalization (In  (None, 8, 8, 512)         1024      \n"," stanceNormalization)                                            \n","                                                                 \n"," leaky_re_lu_1 (LeakyReLU)   (None, 8, 8, 512)         0         \n","                                                                 \n"," conv2d_transpose_1 (Conv2D  (None, 16, 16, 256)       2097152   \n"," Transpose)                                                      \n","                                                                 \n"," instance_normalization_1 (  (None, 16, 16, 256)       512       \n"," InstanceNormalization)                                          \n","                                                                 \n"," leaky_re_lu_2 (LeakyReLU)   (None, 16, 16, 256)       0         \n","                                                                 \n"," conv2d_transpose_2 (Conv2D  (None, 32, 32, 128)       524288    \n"," Transpose)                                                      \n","                                                                 \n"," instance_normalization_2 (  (None, 32, 32, 128)       256       \n"," InstanceNormalization)                                          \n","                                                                 \n"," leaky_re_lu_3 (LeakyReLU)   (None, 32, 32, 128)       0         \n","                                                                 \n"," conv2d_transpose_3 (Conv2D  (None, 64, 64, 64)        131072    \n"," Transpose)                                                      \n","                                                                 \n"," instance_normalization_3 (  (None, 64, 64, 64)        128       \n"," InstanceNormalization)                                          \n","                                                                 \n"," leaky_re_lu_4 (LeakyReLU)   (None, 64, 64, 64)        0         \n","                                                                 \n"," conv2d_transpose_4 (Conv2D  (None, 128, 128, 32)      32768     \n"," Transpose)                                                      \n","                                                                 \n"," instance_normalization_4 (  (None, 128, 128, 32)      64        \n"," InstanceNormalization)                                          \n","                                                                 \n"," leaky_re_lu_5 (LeakyReLU)   (None, 128, 128, 32)      0         \n","                                                                 \n"," conv2d (Conv2D)             (None, 128, 128, 3)       2403      \n","                                                                 \n","=================================================================\n","Total params: 7811363 (29.80 MB)\n","Trainable params: 7811363 (29.80 MB)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n"]}],"source":["inig=keras.initializers.random_normal(0, 0.02)\n","\n","def Build_generator():\n","    model=Sequential()\n","    model.add(Input((100, )))\n","    model.add(Dense(4*4*512, use_bias=True, kernel_initializer=inig))\n","    model.add(LeakyReLU())\n","\n","    model.add(Reshape((4,4,512)))# (4,4,256)\n","\n","    # model.add(UpSampling2D())\n","    model.add(Conv2DTranspose(512, (4,4), (2,2), padding=\"same\", use_bias=False, kernel_initializer=inig))# (8,8,512)\n","    model.add(InstanceNormalization())\n","    model.add(LeakyReLU())\n","\n","\n","    # model.add(UpSampling2D())\n","    model.add(Conv2DTranspose(256, (4,4), (2,2,), padding=\"same\", use_bias=False, kernel_initializer=inig))# (16,16,256)\n","    model.add(InstanceNormalization())\n","    model.add(LeakyReLU())\n","\n","\n","    # model.add(UpSampling2D())\n","    model.add(Conv2DTranspose(128, (4,4), (2,2), padding=\"same\", use_bias=False, kernel_initializer=inig))# (32,32,128)\n","    model.add(InstanceNormalization())\n","    model.add(LeakyReLU())\n","\n","\n","    # model.add(UpSampling2D())\n","    model.add(Conv2DTranspose(64, (4,4), (2,2), padding=\"same\", use_bias=False, kernel_initializer=inig))# (64,64,64)\n","    model.add(InstanceNormalization())\n","    model.add(LeakyReLU())\n","\n","\n","    # model.add(UpSampling2D())\n","    model.add(Conv2DTranspose(32, (4,4), (2,2), padding=\"same\", use_bias=False, kernel_initializer=inig))# (128,128,32)\n","    model.add(InstanceNormalization())\n","    model.add(LeakyReLU())\n","\n","\n","    model.add(Conv2D(3, (5,5), padding=\"same\", activation=\"tanh\", kernel_initializer=inig))#\n","    \n","    return model\n","\n","Build_generator().summary()"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-01-31T07:18:28.745654Z","iopub.status.busy":"2024-01-31T07:18:28.744708Z","iopub.status.idle":"2024-01-31T07:18:28.907115Z","shell.execute_reply":"2024-01-31T07:18:28.906200Z","shell.execute_reply.started":"2024-01-31T07:18:28.745616Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"sequential_1\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv2d_1 (Conv2D)           (None, 64, 64, 32)        1568      \n","                                                                 \n"," leaky_re_lu_6 (LeakyReLU)   (None, 64, 64, 32)        0         \n","                                                                 \n"," conv2d_2 (Conv2D)           (None, 32, 32, 64)        32832     \n","                                                                 \n"," leaky_re_lu_7 (LeakyReLU)   (None, 32, 32, 64)        0         \n","                                                                 \n"," conv2d_3 (Conv2D)           (None, 16, 16, 128)       131200    \n","                                                                 \n"," leaky_re_lu_8 (LeakyReLU)   (None, 16, 16, 128)       0         \n","                                                                 \n"," conv2d_4 (Conv2D)           (None, 8, 8, 256)         524544    \n","                                                                 \n"," leaky_re_lu_9 (LeakyReLU)   (None, 8, 8, 256)         0         \n","                                                                 \n"," conv2d_5 (Conv2D)           (None, 4, 4, 512)         2097664   \n","                                                                 \n"," leaky_re_lu_10 (LeakyReLU)  (None, 4, 4, 512)         0         \n","                                                                 \n"," flatten (Flatten)           (None, 8192)              0         \n","                                                                 \n"," dropout (Dropout)           (None, 8192)              0         \n","                                                                 \n"," dense_1 (Dense)             (None, 1)                 8193      \n","                                                                 \n","=================================================================\n","Total params: 2796001 (10.67 MB)\n","Trainable params: 2796001 (10.67 MB)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n"]}],"source":["inid=keras.initializers.random_normal(0, 0.02)\n","def Build_discrimnator():\n","    model=Sequential()\n","    model.add(Input((128,128,3)))\n","\n","    model.add(Conv2D(32, (4,4), (2,2), padding=\"same\", kernel_initializer=inid))# (64, 64, 32)\n","    model.add(LeakyReLU())\n","\n","    model.add(Conv2D(64, (4,4), (2,2), padding=\"same\", kernel_initializer=inid))# (32, 32, 64)\n","    model.add(LeakyReLU())\n","\n","    model.add(Conv2D(128, (4,4), (2,2), padding=\"same\", kernel_initializer=inid))# (16, 16, 128)\n","    model.add(LeakyReLU())\n","\n","    model.add(Conv2D(256, (4,4), (2,2), padding=\"same\", kernel_initializer=inid))# (8, 8, 256)\n","    model.add(LeakyReLU())\n","\n","    model.add(Conv2D(512, (4,4), (2,2), padding=\"same\", kernel_initializer=inid))# (4, 4, 512)\n","    model.add(LeakyReLU())\n","\n","    model.add(Flatten())\n","    model.add(Dropout(0.25))\n","\n","    model.add(Dense(1))\n","\n","    return model\n","\n","\n","Build_discrimnator().summary()"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-01-31T07:18:55.951325Z","iopub.status.busy":"2024-01-31T07:18:55.950895Z","iopub.status.idle":"2024-01-31T07:18:55.972931Z","shell.execute_reply":"2024-01-31T07:18:55.971993Z","shell.execute_reply.started":"2024-01-31T07:18:55.951293Z"},"trusted":true},"outputs":[],"source":["class DCGAN(keras.Model):\n","    def __init__(self, discriminator, generator, latent_dim):\n","        super().__init__()\n","        self.discriminator = discriminator\n","        self.generator = generator\n","        self.latent_dim = latent_dim\n","        self.seed=tf.random.normal([16,100])\n","        self.Batch_size=64\n","        self.d_loss_tracker = keras.metrics.Mean(name=\"d_loss\")\n","        self.g_loss_tracker = keras.metrics.Mean(name=\"g_loss\")\n","\n","    def compile(self, d_optimizer, g_optimizer):\n","        super().compile()\n","        self.d_optimizer = d_optimizer\n","        self.g_optimizer = g_optimizer\n","\n","\n","\n","    # losses\n","    def discriminator_loss(self, real_img, fake_img):\n","        real_loss = tf.reduce_mean(real_img)\n","        fake_loss = tf.reduce_mean(fake_img)\n","        return fake_loss - real_loss\n","\n","\n","    # Define the loss functions for the generator.\n","    def generator_loss(self, fake_img):\n","        return -tf.reduce_mean(fake_img)\n","\n","\n","    def gradient_penalty(self, batch_size, real_images, fake_images):\n","        \"\"\"Calculates the gradient penalty.\n","\n","        This loss is calculated on an interpolated image\n","        and added to the discriminator loss.\n","        \"\"\"\n","        # Get the interpolated image\n","        alpha = tf.random.uniform([batch_size, 1, 1, 1], 0.0, 1.0)\n","        diff = fake_images - real_images\n","        interpolated = real_images + alpha * diff\n","\n","        with tf.GradientTape() as gp_tape:\n","            gp_tape.watch(interpolated)\n","            # 1. Get the discriminator output for this interpolated image.\n","            pred = self.discriminator(interpolated, training=True)\n","\n","        # 2. Calculate the gradients w.r.t to this interpolated image.\n","        grads = gp_tape.gradient(pred, [interpolated])[0]\n","        # 3. Calculate the norm of the gradients.\n","        norm = tf.sqrt(tf.reduce_sum(tf.square(grads), axis=[1, 2, 3]))\n","        gp = tf.reduce_mean((norm - 1.0) ** 2)\n","        return gp\n","    \n","\n","    def show_pro(self, epoch):\n","        predictions = self.generator(self.seed, training=False)\n","        predictions=(predictions+1)/2.0\n","\n","        fig = plt.figure(figsize=(5, 5))\n","\n","        for i in range(16):\n","            plt.subplot(4, 4, i+1)\n","            plt.imshow(predictions[i])\n","            plt.axis('off')\n","        \n","        plt.savefig('Images/image_at_epoch_{:04d}.png'.format(epoch))\n","        plt.show()\n","        \n","        \n","    def Train_disc(self, Images):\n","        noise = tf.random.normal([self.Batch_size, self.latent_dim])\n","        generated_images = self.generator(noise, training=False)\n","        with tf.GradientTape() as disc_tape2:\n","            real_out=self.discriminator(Images, training=True)\n","            fake_out=self.discriminator(generated_images, training=True)\n","\n","            d_cost = self.discriminator_loss(real_out, fake_out)\n","            gp = self.gradient_penalty(Batch_size, Images, generated_images)\n","            disc_loss= d_cost + gp * 10\n","\n","        gradients_of_discriminator = disc_tape2.gradient(disc_loss, self.discriminator.trainable_variables)\n","        self.d_optimizer.apply_gradients(zip(gradients_of_discriminator, self.discriminator.trainable_variables))\n","\n","\n","\n","\n","\n","    def train_step(self, real_images):\n","        if isinstance(real_images, tuple):\n","            real_images = real_images[0]\n","\n","\n","        self.Batch_size= tf.shape(real_images)[0]\n","        random_latent_vectors = tf.random.normal(shape=(self.Batch_size, self.latent_dim))\n","\n","        with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n","            gen_img=self.generator(random_latent_vectors)\n","\n","            real_out=self.discriminator(real_images)\n","            fake_out=self.discriminator(gen_img)\n","\n","            gen_loss=self.generator_loss(fake_out)\n","            d_cost = self.discriminator_loss(real_out, fake_out)\n","            gp = self.gradient_penalty(self.Batch_size, real_images, gen_img)\n","            disc_loss= d_cost + gp * 10\n","\n","\n","        gradients_of_generator = gen_tape.gradient(gen_loss, self.generator.trainable_variables)\n","        self.g_optimizer.apply_gradients(zip(gradients_of_generator, self.generator.trainable_variables))\n","\n","        gradients_of_discriminator = disc_tape.gradient(disc_loss, self.discriminator.trainable_variables)\n","        self.d_optimizer.apply_gradients(zip(gradients_of_discriminator, self.discriminator.trainable_variables))\n","\n","#       Train discriminator for 5 more epochs \n","        for _ in range(5):\n","            self.Train_disc(real_images)\n","\n","\n","        # Update metrics and return their value.\n","        self.d_loss_tracker.update_state(disc_loss)\n","        self.g_loss_tracker.update_state(gen_loss)\n","        return {\n","            \"d_loss\": self.d_loss_tracker.result(),\n","            \"g_loss\": self.g_loss_tracker.result()\n","        }"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2024-01-31T07:19:06.559747Z","iopub.status.busy":"2024-01-31T07:19:06.559403Z","iopub.status.idle":"2024-01-31T07:19:06.571802Z","shell.execute_reply":"2024-01-31T07:19:06.571077Z","shell.execute_reply.started":"2024-01-31T07:19:06.559715Z"},"trusted":true},"outputs":[],"source":["from IPython import display\n","class CustomCallback(keras.callbacks.Callback):\n","\n","    def __init__(self, ckmanager):\n","        super().__init__()\n","        self.ckpt_manager=ckmanager\n","\n","    def on_train_begin(self, logs=None):\n","        pass\n","\n","    def on_epoch_begin(self, epoch, logs=None):\n","        pass\n","        \n","\n","    def on_epoch_end(self, epoch, logs=None):\n","        display.clear_output(wait=True)\n","        self.model.show_pro(epoch+600)\n","        self.ckpt_manager.save()\n","        print(\"Model Saved\")\n","        \n","    def on_train_batch_begin(self, batch, logs=None):\n","        pass      \n","\n","    def on_train_batch_end(self, batch, logs=None):\n","        pass"]},{"cell_type":"code","execution_count":129,"metadata":{"execution":{"iopub.execute_input":"2024-01-31T09:46:40.568035Z","iopub.status.busy":"2024-01-31T09:46:40.567651Z","iopub.status.idle":"2024-01-31T09:46:40.956255Z","shell.execute_reply":"2024-01-31T09:46:40.955507Z","shell.execute_reply.started":"2024-01-31T09:46:40.568005Z"},"trusted":true},"outputs":[],"source":["with tf.device(\"/device:GPU:0\"):\n","    generator=Build_generator()\n","    discriminator=Build_discrimnator()\n","    DCgan=DCGAN(discriminator, generator, 100)\n","    \n","# generator.load_weights(\"Models/generator_weights.h5\")\n","# discriminator.load_weights(\"Models/discriminator_weights.h5\")\n","\n","opt_gen=Adam(learning_rate=0.00008, beta_1=0.5)\n","opt_desc=Adam(learning_rate=0.00004, beta_1=0.5)\n","DCgan.compile(d_optimizer=opt_desc, g_optimizer=opt_gen)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2024-01-31T07:19:26.222745Z","iopub.status.busy":"2024-01-31T07:19:26.222376Z","iopub.status.idle":"2024-01-31T07:19:26.229585Z","shell.execute_reply":"2024-01-31T07:19:26.228586Z","shell.execute_reply.started":"2024-01-31T07:19:26.222716Z"},"trusted":true},"outputs":[],"source":["\n","# CHECK POINTS\n","checkpoint_path = \"./checkpoints/train\"\n","\n","ckpt = tf.train.Checkpoint(generator=generator,\n","                           discriminator=discriminator,\n","                           generator_optimizer=opt_gen,\n","                           discriminator_optimizer=opt_desc,)\n","\n","ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":true,"trusted":true},"outputs":[],"source":["# if a checkpoint exists, restore the latest checkpoint.\n","if ckpt_manager.latest_checkpoint:\n","  ckpt.restore(ckpt_manager.latest_checkpoint)\n","  print ('Latest checkpoint restored!!')"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2024-01-31T07:19:29.338196Z","iopub.status.busy":"2024-01-31T07:19:29.337266Z","iopub.status.idle":"2024-01-31T07:19:30.177976Z","shell.execute_reply":"2024-01-31T07:19:30.177189Z","shell.execute_reply.started":"2024-01-31T07:19:29.338135Z"},"trusted":true},"outputs":[],"source":["def load(image_path):\n","    img = tf.io.read_file(image_path)\n","    img = tf.io.decode_jpeg(img)\n","    img = tf.image.resize(img, [128, 128])\n","    img = tf.cast(img, tf.float32)\n","    img = (img - 127.5) / 127.5\n","    return img\n","    \n","dataset=tf.data.Dataset.list_files(Dataset_path).map(load).batch(Batch_size, drop_remainder=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-01-31T07:23:58.685190Z","iopub.status.busy":"2024-01-31T07:23:58.684434Z"},"trusted":true},"outputs":[],"source":["# Start Training \n","DCgan.fit(dataset, epochs=600, callbacks=[CustomCallback(ckpt_manager)], verbose=2)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-01-31T10:07:13.157702Z","iopub.status.busy":"2024-01-31T10:07:13.156976Z","iopub.status.idle":"2024-01-31T10:07:13.483774Z","shell.execute_reply":"2024-01-31T10:07:13.482888Z","shell.execute_reply.started":"2024-01-31T10:07:13.157670Z"},"trusted":true},"outputs":[],"source":["#  show some images form dataset\n","iterr=next(dataset.take(1).as_numpy_iterator())\n","print(np.max(iterr[4]))\n","plt.imshow((iterr[4]+1)/2)\n","plt.show()"]},{"cell_type":"code","execution_count":29,"metadata":{"execution":{"iopub.execute_input":"2024-01-31T07:20:30.434948Z","iopub.status.busy":"2024-01-31T07:20:30.434585Z","iopub.status.idle":"2024-01-31T07:20:30.440595Z","shell.execute_reply":"2024-01-31T07:20:30.439607Z","shell.execute_reply.started":"2024-01-31T07:20:30.434919Z"},"trusted":true},"outputs":[],"source":["# random noise for checking model performance\n","noise=tf.random.normal([1, 100])"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-01-31T10:18:32.538555Z","iopub.status.busy":"2024-01-31T10:18:32.537794Z","iopub.status.idle":"2024-01-31T10:18:32.889403Z","shell.execute_reply":"2024-01-31T10:18:32.888476Z","shell.execute_reply.started":"2024-01-31T10:18:32.538522Z"},"trusted":true},"outputs":[],"source":["img=generator(noise, training=False)\n","img=(img[0]+1)/2\n","plt.imshow(img)\n","plt.show()"]},{"cell_type":"code","execution_count":41,"metadata":{"execution":{"iopub.execute_input":"2024-01-30T21:35:01.928931Z","iopub.status.busy":"2024-01-30T21:35:01.928558Z","iopub.status.idle":"2024-01-30T21:35:02.145541Z","shell.execute_reply":"2024-01-30T21:35:02.144652Z","shell.execute_reply.started":"2024-01-30T21:35:01.928900Z"},"trusted":true},"outputs":[],"source":["# save model \n","generator.save(\"Models/gennrator.h5\")\n","discriminator.save(\"Models/discromonator.h5\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-01-31T10:13:01.454924Z","iopub.status.busy":"2024-01-31T10:13:01.453907Z","iopub.status.idle":"2024-01-31T10:13:04.017905Z","shell.execute_reply":"2024-01-31T10:13:04.016953Z","shell.execute_reply.started":"2024-01-31T10:13:01.454886Z"},"trusted":true},"outputs":[],"source":["#  Show The Dataset Images\n","iterr=next(dataset.take(1).as_numpy_iterator())\n","def show_pro(predictions):\n","    predictions=(predictions+1)/2\n","\n","    fig = plt.figure(figsize=(10, 10))\n","\n","    for i in range(len(predictions)):\n","        plt.subplot(8, 8, i+1)\n","        plt.imshow(predictions[i])\n","        plt.axis('off')\n","    plt.show()\n","\n","show_pro(iterr)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":4101010,"sourceId":7112108,"sourceType":"datasetVersion"}],"dockerImageVersionId":30636,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}
